---
layout: post
title:  "Large Scale Graph Mining: Pregel"
date:   2016-12-15 10:00:00 +0100
categories: AI
---
One of the things that has been keeping me busy lately was a paper I had to write
for one of my classes about large-scale graph mining. It's been a lot of work, but it's 
been quite enlightening - I've learnt quite a bit. I will probably upload the paper to my site
as well, once I've finished this seminar, but in the meantime I want to write a bit about Pregel,
since I found it a really interesting concept.

Pregel is Google's architecture for distributed computation of a variety of values pertaining to graphs
and to a certain extent represents their successor to the widely popular MapReduce framework.
In contrast to MapReduce it's designed for a more specific task and also differs quite a bit
with regard to its implementation. The parallelization occurs on a far lower level and unlike the
reduce stage of MapReduce, data and results typically stay local for the duration of the execution.

It essentially operates on the same principle as multi-agent systems, with each node of the graph
representing its own autonomous agent with local data and behavior rules. These node agents are
distributed across a typically large number of computers and use message passing to communicate
between each other when necessary. To avoid race conditions and deadlocks, communication is not
performed at arbitrary times, but rather in globally synchronized fixed intervals called supersteps.
Between communication phases, the individual nodes execute their computations completely asynchronously
without access to external data (neither reads nor writes). During the computation phase, nodes
may enqueue any number of messages to be sent to other nodes, however due to the iterative nature
of the execution, these messages will only be available to their target nodes at the next superstep.

![Schematic of a very small Pregel network. Multiple workers can run on the same machine for better load balancing.](/images/pregel_network.png)

*Schematic of a small Pregel Network. Multiple workers can run on the same machine for better load balancing.*

Pregel is written in C++ and contains all the necessary code to distribute data across different
workers, monitor worker health and roll back to backups when necessary, facilitate communication,
and provide analytics to the user. To implement an algorithm for Pregel, all the user must do is
inherit from the `Vertex` class and override its `compute` function. How to write this function
is however not so straightforward. Many existing graph algorithms rely on access to global data
throughout the execution, and writing a Pregel implementation that sends data from every node
to every other node in each step would thoroughly saturate the I/O bottleneck and defeat the purpose
of the decentralized computing. Instead, typical applications have vertices sending messages only
to their neighbors, which for most sparse graphs derived from real world data is a small number. 

So far this is quite theoretical, and the reader may be wondering how limited-scope agents can
be used to achieve the desired global behavior. For this we first have to look at what exactly
a node is capable of doing. In fact, a node is capable of doing very little, possessing only
a few methods and properties. In pseudo-code, a vertex looks more or less like this:

```
class Vertex:
    function compute(received_messages)
    function vote_to_halt()
    function send_message_to(destination, message)
    var value;
    var out_edges;
    bool active;
    const var superstep;
```

As mentioned previously, the `compute` method is what the user must override for their specific
application and it is passed an iterable containing the incoming messages at each step, and may
call the other methods and modify the instance variables. The `vote_to_halt` method is already
implemented by the parent class, and simply changes the instance's active attribute to False,
which means that in subsequent supersteps the node will do nothing unless reactivated by
incoming messages. Also already implemented is `send_message_to`, although users can specify arbitrary
data types for the `message` itself. Similar to messages, the attribute `value` can also be user defined
depending on the application, and represents some form of local storage. The `out_edges` property
is set externally at initialization and contains a list of edges connected to the node (in the shape
of target ID and weight pairs) and can be changed during runtime. The `active` property indicates 
whether the node has voted to halt and is accessible globally: when all nodes have voted to halt, 
execution terminates. Finally `superstep` is provided externally and basically represents a global 
clock signal.


So, given that one can really only customize the method compute, and the data types for method and
value, the programming paradigm must change significantly from traditional algorithms. To illustrate 
this shift, here is an example of how to implement single source shortest path (SSSP):

* `value`: integer, initalized to INF for all nodes except the source, which starts with 0
* `message`: integer
* `compute`: the following (rather Python-like) pseudocode:

```
compute(received_messages):
    minimum_distance = min(received_messages)
    if minimum_distance < value:
        value = minimum_distance
        for destination, weight in out_edges:
            send_message_to(destination, value + weight)
    vote_to_halt()
```
     
When every vertex has voted to halt, their values will be either the shortest path to the source, or
INF for unreachable nodes. Total number of supersteps is the diameter of the graph, and at each step
a vertex sends at most `len(out_edges)` messages, though often none at all if its value hasn't been
updated since the last send. To my knowledge this is typical of the amount of messages that need to
be sent in a Pregel run, and indicates that computing a global result can be achieved without very
large exchanges of information.

Nonetheless, it is common knowledge that I/O operations are orders of magnitude slower than computations
and especially so when performed over the network (when you consider that even writes/reads from 
SSDs are typically bottlenecks) so there are a certain number of optimizations implemented in Pregel
to reduce the cost of communication. For starters, although each node can send messages to any other
node, which may all be on disparate machines, each worker processes the messages enqueued during a
superstep by its contained nodes and groups them by target location to send in a single bulk message.

Another bandwidth saving measure is the option to combine messages before sending them, according
to a user-specified combiner function. This combiner function must be manually enabled and implemented
depending on the algorithm, as Pregel cannot automatically determine a suitable combiner. In the case 
of SSSP, a combiner would look like:

```
combine(destination, messages_for_same_destination):
    return destination, min(messages_for_same_destination)
```

According to Google, use of combiners reduces bandwidth use by a factor of four in practice. One key
question that I have not answered at all so far is: how does a worker know which machine to send
messages intended for a certain vertex to? How are the vertices distributed across the machines in
the first place? The answer to this is disappointingly primitive: each node has a unique ID, and a
hash function of this ID is modulo'ed with the number of workers to obtain the index of which
machine to place the node on. This same hash and modulo determine where to send messages during
execution. This means that graph structure is not considered at all in the distribution of vertices
and a clique of strongly connected nodes may wind up on different computers (or even different continents
on the scale that Google operates on) and communication between them must be performed over network.

That really made me wonder how this naive solution can be the best option they came up with and how
it can be that it doesn't cause severe bottlenecking by the communication phases. To investigate
this for myself empirically, I set out to recreate Pregel by myself (I know that there are open 
source versions such as Giraph freely available for anybody to download, but I didn't do this for
two reasons: 1. I wanted to get a better understanding of the inner workings and 2. I don't actually
have a server cluster).

So, being a simulation and not designed for high performance, I made it in Python (ergo: sequentially)
since it's easy to use; and since I wanted introspection, I made a graphical representation of
the system state in Qt5 (simply because that's what I know and I didn't want to worry about learning
something new). The project ended up getting surprisingly big, so I decided I might as well put it
on github. The code should probably still get cleaned up a bit, and since nobody downloads software
anymore these days, I want to look into making it a web-app (yay, web 2.0 :/) as well. I'll go into
more detail about this simulation and the insights gleaned from it in a future post.